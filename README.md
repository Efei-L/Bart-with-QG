# Bart-with-QG
BART is a powerful pre-trained model that has excelled in generative tasks such as text summarization, question answering, and machine translation. In previous studies,the BART model has often been used for multi-hop question generation task(MQG), and it significantly improved the quality of generated questions compared to recurrent neural network-based models. However, due to the differences between downstream tasks and pre-training tasks, BART still generates some nonsensical and grammatically incorrect questions in multi-hop question generation tasks. These types of questions can have a negative impact on the user's reading experience. To address this challenge, we propose a BART-based retouching framework(BRQG), which builds upon BART. Specifically, BRQG uses BART-generated questions as a starting point, and introduces a Retouching Network module to reattend to the questions and context. The Retouching Gate layer then fuses this attention in an appropriate proportion to generate second-round questions that are more complete and readable. In addition, we propose a Entity Awareness Enhancement module, which construct graph structures from input documents to improve the correctness of entity generation. We conducted experiments on the HotpotQA dataset, and the results show that our model outperforms the currently proposed model on BLEU4, demonstrating the advantages and feasibility of BRQG in multi-hop question generation.

<img width="570" alt="image" src="https://github.com/Efei-L/Bart-with-QG/assets/74166207/fa6d846c-03e3-4982-b47e-ebc0073c6839">
<img width="589" alt="image" src="https://github.com/Efei-L/Bart-with-QG/assets/74166207/634e1f1d-d20e-4e53-8cf3-7f5e1ad47f0c">
<img width="478" alt="image" src="https://github.com/Efei-L/Bart-with-QG/assets/74166207/bdf5b14e-31f2-4320-82e4-49d421128dfc">
<img width="570" alt="image" src="https://github.com/Efei-L/Bart-with-QG/assets/74166207/f7305e61-bbd6-44ac-9387-f13106ec26aa">
<img width="598" alt="image" src="https://github.com/Efei-L/Bart-with-QG/assets/74166207/55d09671-1503-444d-b161-99c6abb194ba">
<img width="602" alt="image" src="https://github.com/Efei-L/Bart-with-QG/assets/74166207/9247c958-9bcc-4924-a96a-ab69d1687126">

In this paper, we propose the BRQG framework to enhance the quality of BART-generated questions. The framework comprises two additional modules, namely the Entity Awareness Enhancement Module and Retouching Network Module, integrated with the BART model. Our experiments show that the Retouching Network is highly effective in improving the quality of questions generated by BART, including aspects like the distribution of question length and the use of accurate question words. Additionally, the Entity Awareness Enhancement Module successfully enhances the correctness rate of entities generated by BRQG, when compared to BART. Finally, we demonstrate the effectiveness of Retouching Gate, which allocates attention effectively and helps the model focus on relevant information. 
In theory, our model is expected to perform well on generative language models such as BART and T5. However, for the current study, we only conducted experiments on the BART model. In future work, we plan to extend our experiments to include a wider range of models.
We sincerely hope that our work will be useful for subsequent research on MQG.
